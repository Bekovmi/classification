model_params:
  fp16: false       # general flag
  model: baseline
  encoder_params:
    arch: resnet18
    pretrained: True
    frozen: True
    pooling: GlobalConcatPool2d
  head_params:
    hiddens: [1024]
    emb_size: 64
    n_cls: 2
    activation_fn: ReLU
    norm_fn: BatchNorm1d
    bias: false
    dropout: 0.5

args:
  expdir: "finetune"
  baselogdir: "./logs/finetune"

stages:

  state_params:
    main_metric: &reduce_metric precision01
    minimize_metric: False

  criterion_params:
    criterion: CrossEntropyLoss

  scheduler_params:
    scheduler: MultiStepLR
    milestones: [8]
    gamma: 0.3

  data_params:
    n_workers: 4
    in_csv: "./data/ants_bees/dataset.csv"
    tag2class: "./data/ants_bees/tag2cls.json"
    tag_column: "tag"
    class_column: "class"
    n_folds: 5
    train_folds: [0, 1, 2, 3]
    datapath: "./data/ants_bees/"

  callbacks_params:
    loss:
      callback: EmbeddingsLossCallback
      emb_l2_reg: -1
      input_key: targets
      embeddings_key: embeddings
      logits_key: logits
    optimizer:
      callback: OptimizerCallback
    precision:
      callback: PrecisionCallback
      precision_args: [1]
    scheduler:
      callback: SchedulerCallback
      reduce_metric: *reduce_metric
    saver:
      callback: CheckpointCallback

  # train head
  stage1:
    state_params:
      n_epochs: 10
    data_params:
      batch_size: 64

    optimizer_params:
      optimizer: Adam
      lr: 0.001
      weight_decay: 0.0001

  # tune whole network
  stage2:
    state_params:
      n_epochs: 5
    data_params:
      batch_size: 64

    optimizer_params:
      optimizer: SGD
      lr: 0.0001
